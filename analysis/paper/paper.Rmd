---
title: "A Standard for the Scholarly Citation of Archaeological Data: Supplementary Information"
author:
  - "Ben Marwick"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::html_document2:
    fig_caption: yes
    reference_docx: templates/template.docx
bibliography: references.bib
csl: journal-of-archaeological-science.csl
abstract: |
  Text of abstract
keywords: |
  keyword 1; keyword 2; keyword 3
highlights: |
  These are the highlights. 
---

This document contains code and text supporting the results presented in "A Standard for the Scholarly Citation of Archaeological Data" (draft online at https://docs.google.com/document/d/1UT4RuW4SCkdLGsn2UmNK22vgfJZxljg8ugRstjToCN8/edit).

The source file for this document, and other files in the compendium are online at https://github.com/benmarwick/datacitation

<!-- This is the format for text comments that will be ignored during renderings. Do not put R code in these comments because it will not be ignored. -->

```{r, setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  eval = FALSE,
  comment = "#>",
  fig.path = "../figures/"
)

library(datacitation)

# my custom plot theme
theme_new <- function(base_size = 12){
  theme_bw(base_size = base_size) %+replace%
    theme(
      #axis.title.x=element_blank(),
      # axis.title.y=element_blank(),
      axis.title = element_text(size = 14),
      legend.key=element_rect(colour=NA, fill =NA),
      # panel.grid = element_blank(),   
      panel.border = element_rect(fill = NA, colour = "black", size=0.5),
      panel.background = element_rect(fill = "white", colour = "black"), 
      strip.background = element_rect(fill = NA)
    )
}

```

# Email survey of data sharing behaviours

```{r eval = TRUE}
library(tidyverse)
email_survey <- readr::read_csv("../data/raw_data/email_survey.csv")

# overall rate of response
emails_out <- sum(!is.na(email_survey$`email sent date`))
emails_back <- sum(!is.na(email_survey$`response received date`))
email_response_rate <- round(emails_back / emails_out, 1) * 100
data_items_received <-  sum(!is.na(email_survey$`content recieved date`))
sharing_rate <-  round(data_items_received / emails_out, 1) * 100
```

Our sample focused on _Journal of Archaeological Science_ articles on experiments with stone artefacts that were published during `r min(email_survey$'publication year', na.rm = TRUE)` and `r max(email_survey$'publication year', na.rm = TRUE)`. We focused on stone artefact experiments because the all research materials in these projects are generated by the researcher, so there are no other stakeholders to consider when determining how to share the data: the researcher has complete control over the data. There are also no risks of damage to people or property in sharing these data because they do not include archaeological sites or human subjects. In contacting the authors, we requested the datasets for use in a graduate seminar with the aim of reproducing the published results and further exploring the data (this stage is still in progress). We promised to contact the authors if we discovered anything new, and not to share the data further without their permission. The data included with this paper is anonymised so the specific authors we contacted are not identifiable. 

We sent out emails to the first authors of `r emails_out` stone artefact experiment papers, and recieved `r emails_back` replies, resulting in a `r email_response_rate`% response rate. We recieved `r data_items_received` responses that included data files, giving an overall sharing rate of `r sharing_rate`%. Our small sample size limits the robustness of our analysis, but points to some observations that may be worth following up in a larger study. We found no clear relationship of data sharing and date of publication. We do not have any insights into why authors of older papers decline to share, but three of the six responses for papers published in 2014-2015 said they declined to share data because they intend to use these data in a future publication. Other noteworthy reasons provided by respondents for declining to share include the author about to defend their thesis or about to go on vacation. We interpret these to mean that the author does not have time to prepare their data for sharing, perhaps because the data are not already in a well-organised for ready for distribution. We found no effect on the author's student status at time of publication on data sharing behaviours, with roughly equal proportions of students and non-students sharing and not sharing. Similarly there are no clear patterns data sharing of among different journals. It is remarkable that the authors of the one PLOS One paper we contacted declined to share because that refusal violates the policy of that journal at the time of publication of that paper (early 2014): "Publication is conditional upon the agreement of the authors to make freely available any materials and information described in their publication that may be reasonably requested by others for the purpose of academic, non-commercial research."   

```{r date-and-sharing, fig.cap="There is no obvious trend in data sharing becavious and date of publication", eval = TRUE}
library(viridis)
years <- seq(min(email_survey$'publication year', na.rm = TRUE),
    max(email_survey$'publication year', na.rm = TRUE), 1)
email_survey %>% 
  filter(!is.na( `email sent date`)) %>% 
  group_by(`publication year`, `content recieved date`) %>% 
  tally() %>% 
  filter(!is.na(`publication year`)) %>% 
  mutate(`content recieved date` = if_else(!is.na(`content recieved date`), 
  "yes", "no")) %>% 
  ggplot(aes(`publication year`, n)) +
    geom_col(aes(fill = `content recieved date`),
             position = position_dodge(preserve = 'single')) +
    scale_fill_viridis(discrete = TRUE,
                       guide = guide_legend(title = "Data shared?")) +
    scale_x_continuous(breaks = years,
                       labels = years) +
    theme_bw()

```

```{r student, fig.cap="No relationship between student status and data sharing behaviour", eval = TRUE}
email_survey %>% 
  filter(!is.na( `email sent date`)) %>% 
    mutate(`content recieved date` = if_else(!is.na(`content recieved date`), 
  "yes", "no")) %>% 
  group_by(
  `author is graduate student at time of publication?`, 
  `content recieved date`) %>% 
  tally() %>% 
  filter(!is.na(`author is graduate student at time of publication?`)) %>% 
  ggplot(aes(`author is graduate student at time of publication?`, n)) +
    geom_col(aes(fill = `content recieved date`),
             position = position_dodge(preserve = 'single')) +
    scale_fill_viridis(discrete = TRUE,
                       guide = guide_legend(title = "Data shared?")) +
    theme_bw()
```

```{r author-country, eval = TRUE}
email_survey %>% 
  filter(!is.na( `email sent date`)) %>% 
    mutate(`content recieved date` = if_else(!is.na(`content recieved date`), 
  "yes", "no")) %>% 
  group_by(
  `author home`, 
  `content recieved date`) %>% 
  tally() %>% 
  filter(!is.na(`author home`)) %>% 
  ggplot(aes(`author home`, n)) +
    geom_col(aes(fill = `content recieved date`),
             position = position_dodge(preserve = 'single')) +
    scale_fill_viridis(discrete = TRUE,
                       guide = guide_legend(title = "Data shared?")) +
    theme_bw()
```


```{r journals, fig.cap="No relationship between sharing behaviour and journal.", eval = TRUE}
email_survey %>% 
  filter(!is.na( `email sent date`)) %>% 
    mutate(`content recieved date` = if_else(!is.na(`content recieved date`), 
  "yes", "no")) %>% 
  group_by(
  journal, 
  `content recieved date`) %>% 
  tally() %>% 
  filter(!is.na(journal)) %>% 
  ggplot(aes(journal, n)) +
    geom_col(aes(fill = `content recieved date`),
             position = position_dodge(preserve = 'single')) +
    scale_fill_viridis(discrete = TRUE,
                       guide = guide_legend(title = "Data shared?")) +
    theme_bw()
```

# Survey of articles published in JAS

```{r eval = TRUE}
jas_article_survey <- readr::read_csv("../data/raw_data/jas_article_survey.csv")

have_data_avail_statement <- nrow(jas_article_survey[jas_article_survey$`Data availability statement` != "none", ])
have_data_avail_statement_perc <- round(have_data_avail_statement / nrow(jas_article_survey) * 100,0)
data_are_avail <- nrow(jas_article_survey[jas_article_survey$`Can I access the data without contacting the authors?` %in% c("Yes", "yes"), ])
data_are_avail_perc <- round(data_are_avail / nrow(jas_article_survey) * 100,0)

data_in_table <-  nrow(jas_article_survey[grepl("Tab", jas_article_survey$`Where is the data?`), ])
data_in_table_perc <- round(data_in_table / data_are_avail * 100, 0)

data_in_supp <-  nrow(jas_article_survey[grepl("Sup", jas_article_survey$`Where is the data?`), ])
data_in_supp_perc <- round(data_in_supp / data_are_avail * 100, 0)

data_in_repo <-  nrow(jas_article_survey[grepl("www|dryad|osf", tolower(jas_article_survey$`Where is the data?`)), ])
data_in_repo_perc <- round(data_in_repo / data_are_avail * 100, 0)

data_type <- jas_article_survey %>% 
  filter(`data type?` != "na") %>% 
  group_by(`data type?`) %>% 
  tally(sort = TRUE) %>% 
  mutate(perc = round(n / sum(n) *100,1))

comp_n <- data_type %>% 
  filter(`data type?`  == "compositional") %>% 
  pull(n)

comp_perc <- data_type %>% 
  filter(`data type?`  == "compositional") %>% 
  pull(perc)

dna_n <- data_type %>% 
  filter(`data type?`  == "DNA sequence") %>% 
  pull(n)

dna_perc <- data_type %>% 
  filter(`data type?`  == "DNA sequence") %>% 
  pull(perc)

dates_n <- data_type %>% 
  filter(grepl("dates", `data type?`)) %>% 
  summarise(n = sum(n)) %>% 
  pull(n)

dates_perc <- data_type %>% 
  filter(grepl("dates", `data type?`)) %>% 
  summarise(n = sum(n),
            perc = sum(perc)) %>% 
  pull(perc)

file_type <- jas_article_survey %>% 
  filter(`What file format?` != "na") %>% 
  group_by(`What file format?`) %>% 
  tally(sort = TRUE) %>% 
  mutate(perc = round(n / sum(n) *100,1))

xlsx_n <- file_type %>% 
  filter(grepl("XLSX", `What file format?`)) %>% 
  summarise(n = sum(n),
            perc = sum(perc)) %>% 
  pull(n)

xlsx_perc <- file_type %>% 
  filter(grepl("XLSX", `What file format?`)) %>% 
  summarise(n = sum(n),
            perc = sum(perc)) %>% 
  pull(perc)

csv_n <- file_type %>% 
  filter(grepl("CSV", `What file format?`)) %>% 
  summarise(n = sum(n),
            perc = sum(perc)) %>% 
  pull(n)

csv_perc <- file_type %>% 
  filter(grepl("CSV", `What file format?`)) %>% 
  summarise(n = sum(n),
            perc = sum(perc)) %>% 
  pull(perc)

doc_n <- file_type %>% 
  filter(grepl("doc|DOC", `What file format?`)) %>% 
  summarise(n = sum(n),
            perc = sum(perc)) %>% 
  pull(n)

doc_perc <- file_type %>% 
  filter(grepl("doc|DOC", `What file format?`)) %>% 
  summarise(n = sum(n),
            perc = sum(perc)) %>% 
  pull(perc)

software <- jas_article_survey %>% 
  filter(`Analysis software?` != "na") %>% 
  group_by(`Analysis software?`) %>% 
  tally(sort = TRUE) %>% 
  mutate(perc = round(n / sum(n) *100,1))

script_n <- software %>% 
  filter(grepl("^R$|.*R$|Python|MATLAB|OxCal", `Analysis software?`)) %>% 
  summarise(n = sum(n),
            perc = sum(perc)) %>% 
  pull(n)

script_perc <- software %>% 
  filter(grepl("^R$|.*R$|Python|MATLAB|OxCal", `Analysis software?`)) %>% 
  summarise(n = sum(n),
            perc = sum(perc)) %>% 
  pull(perc)

```

In addition to our email survey, we also conducted a pilot study of a random sample of `r nrow(jas_article_survey)` articles published during 2016-2017 in the _Journal of Archaeological Science_ to investigate data sharing behaviours. In this sample we found openly available raw data in `r data_are_avail` papers (`r data_are_avail_perc`%), even though only `r have_data_avail_statement` articles (`r have_data_avail_statement_perc`%) include a data availability statement. For papers where data was available, `r data_in_supp` papers (`r data_in_supp_perc`%) have data contained in supplementary files published with the paper, `r data_in_table` (`r data_in_table_perc`%) have all the raw data in tables in the text of the paper, and `r data_in_repo` (`r data_in_repo_perc`%) have data in an online repository. The most frequently shared type of data is compositional (e.g. element concentrations, n = `r comp_n`, `r comp_perc`%), followed by radiocarbon and luminescence age data (where the dates are the primary object of analysis, n = `r dates_n`, `r dates_perc`%) and DNA sequences (n = `r dna_n`, `r dna_perc`%). Generally, compositional data is presented in tables in the text, dating data is presented in supplementary files, and DNA data are in repositories (as is typical for DNA data). Data shared in supplementary files and online repositories were most often in Microsoft Excel format (n = `r xlsx_n`, `r xlsx_perc`% of the files in supplements files and repositories), followed by CSV (comma separated variables, a open source plain text spreadsheet format, n = `r csv_n`, `r csv_perc`%), and tables embedded in Microsoft Word or PDF documents (n = `r doc_n`, `r doc_perc`%). When a scripting language such as R, Python, MATLAB or OxCal was used as the analysis software for a paper (n = `r script_n`, `r script_perc`% of the papers where would identify the software), only 2 papers also provided the script with the paper. 


```{r eval=TRUE, fig.cap="No relationship between data file type and data sharing location"}
# correlation of data type and location of sharing
file_type_location <- 
jas_article_survey %>% 
  mutate(file = strsplit(as.character(`What file format?`), ",")) %>% 
  unnest(file) %>% 
  filter(file != "na" & `Where is the data?` != "na") %>% 
  select(file,  `Where is the data?`) %>% 
  mutate(location = case_when(
    grepl("sup", tolower(`Where is the data?`)) ~ "Supplementary",
    grepl("tab", tolower(`Where is the data?`)) ~ "Table",
    grepl("www|dryad|osf", tolower(`Where is the data?`)) ~ "Repository",
  )) %>% 
  mutate(file = case_when(
    grepl("DOCX|doc", file) ~ "DOCX",
    grepl("XLSX| XLSX", file) ~ "XLSX",
    TRUE ~ file
    ))

file_type_location %>% 
  group_by(file, location) %>% 
  tally() %>% 
  ggplot(aes(file, location)) + 
  geom_point(aes(size = n),
             colour = "grey80") +
  theme_bw() + 
  xlab("") + 
  ylab("") +
  scale_size_continuous(range=c(5,25)) + 
  geom_text(aes(label = n)) +
  theme(legend.position = "none") 
```

```{r eval=TRUE, fig.cap="Plot of the relationship between data type and location where it is shared."}
# correlation of data type and location of sharing
data_type_location <- 
jas_article_survey %>% 
  filter(`data type?` != "na" & `Where is the data?` != "na") %>% 
  select(`data type?`, `Where is the data?`) %>% 
  mutate(data_type = case_when(
    grepl("comp", tolower(`data type?`)) ~ "Compositional",
    grepl("c14|osl", tolower(`data type?`)) ~ "Dating",
    grepl("dna", tolower(`data type?`)) ~ "DNA",
    TRUE ~ `data type?`
  )) %>% 
   mutate(location = case_when(
    grepl("sup", tolower(`Where is the data?`)) ~ "Supplementary",
    grepl("tab", tolower(`Where is the data?`)) ~ "Table",
    grepl("www|dryad|osf", tolower(`Where is the data?`)) ~ "Repository"
  )) 

data_type_location %>% 
  group_by(data_type, location) %>% 
  tally() %>% 
  filter(n >= 2) %>% 
  ggplot(aes(data_type, location)) + 
  geom_point(aes(size = n),
             colour = "grey80") +
  theme_bw() + 
  xlab("") + 
  ylab("") +
  scale_size_continuous(range=c(5,25)) + 
  geom_text(aes(label = n)) +
  theme(legend.position = "none") 
```




# Data sets in repositories

```{r eval = TRUE}
# install.packages("rdatacite")

# Original data aquisition on 14 Aug 2017
# library(rdatacite)
# dc_arch <- dc_search(q = "archaeology", rows = pretty(100000)[2])

# Saved locally then uploaded to osf.io/tsm89 (too big for GitHub)
# saveRDS(dc_arch, "dc_arch.rds")
# dc_arch <- readRDS("C:\\emacs\\dc_arch.rds")

# Download from osf.io with this:
# library(osfr)
# download_file("tsm89", private = TRUE)
dc_arch <- readRDS("../data/raw_data/dc_arch.rds")
```


```{r eval = TRUE}
library(tidyverse)
library(stringr)
# glimpse(dc_arch)

dc_arch <- 
dc_arch %>% 
mutate(the_text =  toupper(gsub(" |_|\\(|\\)", "-", rights)),
       the_text = gsub("(\\-)\\1+", "\\1", the_text),
       the_text = gsub("-$", "", the_text)) %>% 
  mutate(cc = case_when(
    grepl("SEMANTICS/OPENACCESS", the_text) ~ "EU Open Access",
    grepl("SEMANTICS/RESTRICTEDACCESS", the_text) ~ "EU Restricted Access",
    grepl("SEMANTICS/CLOSEDACCESS", the_text) ~ "EU Closed Access",
    grepl("CC-BY-[^NC-ND]|CC-BY$|ATTRIBUTION-LICENSE|ATTRIBUTION-[^NON]", the_text) ~ "CC-BY",
    grepl("CC-BY-NC-[^ND]|BY-NC-[^ND]|CC-NC$|ATTRIBUTION-NON-COMMERCIAL-", the_text) ~ "CC-BY-NC",
    grepl("CC-BY-NC-SA-[^ND]|CC-NC-SA$|ATTRIBUTION-NON-COMMERCIAL-SHARE", the_text) ~ "CC-BY-NC-SA",
    grepl("CC-BY-NC-ND|BY-NC-ND|ATTRIBUTION-NONCOMMERCIAL-NO-DERIVATIVE|ATTRIBUTION-NONCOMMERCIAL-NODERIVATIVES|ATTRIBUTION-NONCOMMERCIAL-NODERIVS|NAMENSNENNUNG-NICHTKOMMERZIELL-KEINEBEARBEITUNG|NAMENSNENNUNG-KEINE-KOMMERZIELLE-NUTZUNG", the_text) ~ "CC-BY-NC-ND",
    grepl("ADS|ARCHAEOLOGYDATASERVICE", the_text) ~ "ADS",
    grepl("CC0|CC-0|ZERO", the_text) ~ "CC-0"
  )) %>% 
  mutate(yyyy_date = as.numeric(substr(date, 1,4)),
         yyyy_created = as.numeric(substr(created, 1,4))) %>% 
  mutate(yyyy_date = if_else(is.na(yyyy_date), as.numeric(publicationYear), yyyy_date)) %>% 
  filter(yyyy_date > 1500) %>% 
  filter(yyyy_date < 2018) %>% 
  filter(!grepl("PANGAEA|Global Biodiversity", datacentre)) # pesky Mediterranean Data Archaeology and Rescue & Danish plants!
```


```{r eval = TRUE}
# select cols of interest
xx <- 
dc_arch %>% 
  select( datacentre,
          allocator ,
          date,
          creator,
          description,
          language,
          contributor,
          rights,
          subject,
          format,
          publisher) 

# tallies
datacentre <-  arrange(as_data_frame(table(xx[ , "datacentre"])), desc(n))
allocator  <-  arrange(as_data_frame(table(xx[ , "allocator"])), desc(n))
dates <-       arrange(as_data_frame(table(xx[ , "date"])), desc(n))
creator <-     arrange(as_data_frame(table(xx[ , "creator"])), desc(n)) 
language <-    arrange(as_data_frame(table(xx[ , "language"])), desc(n))  
contributor <- arrange(as_data_frame(table(xx[ , "contributor"])), desc(n)) 
rights <-      arrange(as_data_frame(table(xx[ , "rights"])), desc(n)) 
publisher  <-  arrange(as_data_frame(table(xx[ , "publisher"])), desc(n)) 
file_format <-  arrange(as_data_frame(table(xx[ , "format"])), desc(n))
```

```{r first-dates, eval = TRUE}
dates <- 
  dc_arch 

# when was the first record created?
first_created <- min(dates$yyyy_created)
first_date <-  min(dates$yyyy_date, na.rm = TRUE)

# when is the break out
dates_tally <- 
  dates %>% 
  group_by(yyyy_date) %>% 
  tally()

# when is that big peak, it's 2009
library(changepoint)
out <- cpt.meanvar(dates_tally$n, test.stat = "Poisson", method = "BinSeg")
breakouts <- dates_tally[cpts.ts(out), ]

# who is the top data source in each year?
top_data_sources <- 
dates %>% 
  group_by(datacentre) %>% 
  tally(sort = TRUE) %>% 
  slice(1:10) %>% 
  pull(datacentre)

top_data_source_each_year <- 
dates %>% 
  group_by(datacentre, yyyy_date) %>% 
  tally() %>%  
  ungroup(datacentre) %>% 
  group_by(yyyy_date) %>% 
  slice(which.max(n)) %>% 
  arrange(desc(yyyy_date))


 years_of_interest <- c(2009)
 ymax <- 10000
 
date_plot <- 
  dates  %>% 
  filter(!is.na(yyyy_date)) %>% 
  ggplot(aes(yyyy_date,
             fill = datacentre
             )) +
  geom_bar() +
  # guides(fill=FALSE) +
  scale_fill_viridis(discrete = TRUE,
                     breaks = top_data_sources[1:4],
                     labels = top_data_sources[1:4]) +
  theme_new() +
  theme(legend.position = c(0.3, 0.75)) +
  xlim(c(1950,2017)) +
  xlab("Year data was collected") +
  ylab("Number of archaeology DOIs\nknown to DataCite") +
  geom_vline(xintercept = years_of_interest, colour = "red") +
  annotate("text", 
           x = years_of_interest + 2,
           y = ymax,
           label = years_of_interest) 

# plotly::ggplotly(date_plot2) %>%
#   layout(showlegend = FALSE)

# show plot 
date_plot

# change in each major repo over time
dates  %>% 
  filter(!is.na(yyyy_date)) %>% 
  group_by(yyyy_date, datacentre)  %>% 
  tally() %>% 
  filter(n != 0,
         datacentre %in% top_data_sources[1:9]) %>% 
   ggplot(aes(yyyy_date,
              n
             )) +
  geom_line(size = 1)  +
  scale_color_viridis(discrete = TRUE) +
  scale_y_log10() +
  xlim(c(1990, 2017)) +
  theme_new() +
  facet_wrap(~ datacentre)
```

```{r eval = FALSE}
# text mine a little
library(tidytext)
subject_1gram <- 
xx %>% 
  unnest_tokens(word, subject) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) 

subject_2gram <- 
xx %>% 
  unnest_tokens(bigram, subject, token = "ngrams", n = 2) %>%
  count(word, sort = TRUE) 

subject_3gram <- 
xx %>% 
  unnest_tokens(trigram, subject, token = "ngrams", n = 3) %>%
  count(word, sort = TRUE) 
```

```{r file-types, eval = TRUE}
# File types

# how many records with file type info?
file_types <- c("pdf|image|imagejpeg|jpg|tiff|png|gif|bmp|ai|illustrator|access|mdb|dbf|dbase|accdb|dbase|doc|rtf|word|arcgis|shape|shp|mapinfo|mid|mif|surfer|xls|excel|csv|spreadsheet|spreadsheet|ply|archeolink|autocad|dwg")

number_of_records_with_file_type <- 
file_format %>% 
  filter(grepl(file_types, tolower(Var1))) %>% 
  summarise(sum = sum(n)) %>% 
  pull(sum)

perc_of_records_with_file_type <- round(number_of_records_with_file_type / nrow(dc_arch) * 100, 0)

# how many that are only PDFs? 
have_a_pdf_in_there <- 
file_format %>% 
   filter(grepl("pdf", tolower(Var1))) %>% 
   mutate(Var1 = tolower(gsub("application/|/", "", Var1)) )

perc_have_a_pdf <- round(sum(have_a_pdf_in_there$n) / number_of_records_with_file_type * 100, 0)

n_only_pdf <-  have_a_pdf_in_there %>% 
    group_by(Var1) %>% 
    tally(sort = TRUE) %>% 
    filter(Var1 %in% c("pdf", "pdf,pdf", "pdfx,pdf", "pdfx")) %>% 
    summarise(nnn = sum(nn)) %>% 
    pull(nnn)

perc_only_pdf <-  round(n_only_pdf / number_of_records_with_file_type * 100, 0)

# how many that are only images?
have_an_image_in_there <-
file_format %>% 
   filter(grepl("image|jpg|tiff|png|gif|bmp|ai|illustrator|imagejpeg", tolower(Var1))) %>% 
   mutate(Var1 = tolower(gsub("application/|/", "", Var1)) )

n_only_image <-  have_an_image_in_there %>% 
    group_by(Var1) %>% 
    tally(sort = TRUE) %>% 
    filter(Var1 %in% c("image", "jpg", "tiff", "png", "gif", "bmp", "ai", "illustrator", "imagejpeg", "imagetiff", "imagepng")) %>% 
    summarise(nnn = sum(nn)) %>% 
    pull(nnn)

perc_only_image <-  round(n_only_image / number_of_records_with_file_type * 100, 0)

# how many records contain a structured data file (csv, xls, dbf, etc)?
have_a_structured_data_file_in_there <- 
  file_format %>% 
  filter(grepl("access|mdb|dbf|dbase|accdb|dbase|arcgis|shape|shp|mapinfo|mid|mif|surfer|xls|excel|csv|spreadsheet|spreadsheet", tolower(Var1))) %>% 
   mutate(Var1 = tolower(gsub("application/|/", "", Var1)) )

n_structured_data_file <- have_a_structured_data_file_in_there %>% 
  group_by(Var1) %>% 
  tally(sort=TRUE) %>% 
  summarise(nnn = sum(nn)) %>% 
  pull(nnn)

perc_structured_data_file <- round(n_structured_data_file / number_of_records_with_file_type * 100, 0)

# count each file type once, even for records with multiple files types
file_format_single <- 
file_format %>% 
  filter(grepl(file_types, tolower(Var1))) %>% 
  mutate(file_format = tolower(gsub("application|/", "", Var1))) %>% 
  mutate(file_format_single = strsplit(as.character(file_format), ",|;")) %>% 
  unnest(file_format_single) 

# how many unique types?
how_many_unique_file_types <- 
file_format_single %>% 
  group_by(file_format_single) %>% 
  tally(sort = TRUE)
  
file_format_simple <- 
file_format_single %>% 
  mutate(file_type = case_when(
    grepl("pdf", file_format_single) ~ "pdf",
    grepl("image|imagejpeg|jpg|tiff|png|gif|bmp|ai|illustrator", file_format_single) ~ "image",
    grepl("access|mdb|dbf|dbase|accdb|dbase", file_format_single) ~ "database",
    grepl("doc|rtf|word", file_format_single) ~ "doc",
    grepl("arcgis|shape|shp|mapinfo|mid|mif|surfer", file_format_single) ~ "GIS",
    grepl("xls|excel|csv|spreadsheet", file_format_single) ~ "spreadsheet",
    grepl("ply", file_format_single) ~ "3D",
    grepl("archeolink", file_format_single) ~ "archeolink",
    grepl("autocad|dwg", file_format_single) ~ "autocad"
  ))  

file_format_simple_tally <- 
file_format_simple %>% 
  filter(!is.na(file_type)) %>% 
  group_by(file_type) %>% 
  tally(sort = TRUE) %>% 
  mutate(perc = round(nn / sum (nn) * 100, 0))

file_format_combos_tally <- 
file_format_simple %>% 
  filter(!is.na(file_type)) %>% 
  group_by(n) %>% 
  filter(n_distinct(file_type) >= 2) %>% 
  slice(1) %>% # look at the file_format col only
  group_by(file_format) %>% 
  tally(sort = TRUE)

# which group has the highest *proportion* of structured data files?
proportion_structured_data_files <- 
dc_arch %>% 
    mutate(strutured_data = ifelse(grepl("access|mdb|dbf|dbase|accdb|dbase|arcgis|shape|shp|mapinfo|mid|mif|surfer|xls|excel|csv|spreadsheet|spreadsheet", tolower(format)), "has_structured_data", "no")) %>% 
  group_by(datacentre, strutured_data) %>% 
  tally(sort = TRUE) %>% 
  spread(strutured_data, n) %>% 
  mutate(perc_structured = has_structured_data / (has_structured_data + no) * 100) %>% 
  arrange(desc(perc_structured))

perc_struc_in_ads <- 
  proportion_structured_data_files %>% 
  filter(grepl("ADS", datacentre)) %>% 
  pull(perc_structured) %>% 
  round(., 2)

n_struc_in_ads <- 
  proportion_structured_data_files %>% 
  filter(grepl("ADS", datacentre)) %>% 
  pull(has_structured_data) 

perc_struc_in_dans <- 
  proportion_structured_data_files %>% 
  filter(grepl("DANS", datacentre)) %>% 
  pull(perc_structured) %>% 
  round(., 0)

n_struc_in_dans <- 
  proportion_structured_data_files %>% 
  filter(grepl("DANS", datacentre)) %>% 
  pull(has_structured_data)

```

```{r rights, eval = TRUE}
rights_summary <- 
  rights %>% 
  mutate(the_text =  toupper(gsub(" |_|\\(|\\)", "-", Var1)),
         the_text = gsub("(\\-)\\1+", "\\1", the_text),
         the_text = gsub("-$", "", the_text)) %>% 
  mutate(cc = case_when(
    grepl("SEMANTICS/OPENACCESS", the_text) ~ "EU Open Access",
    grepl("SEMANTICS/RESTRICTEDACCESS", the_text) ~ "EU Restricted Access",
    grepl("SEMANTICS/CLOSEDACCESS", the_text) ~ "EU Closed Access",
    grepl("CC-BY-[^NC-ND]|CC-BY$|ATTRIBUTION-LICENSE|ATTRIBUTION-[^NON]", the_text) ~ "CC-BY",
    grepl("CC-BY-NC-[^ND]|BY-NC-[^ND]|CC-NC$|ATTRIBUTION-NON-COMMERCIAL-", the_text) ~ "CC-BY-NC",
    grepl("CC-BY-NC-SA-[^ND]|CC-NC-SA$|ATTRIBUTION-NON-COMMERCIAL-SHARE", the_text) ~ "CC-BY-NC-SA",
    grepl("CC-BY-NC-ND|BY-NC-ND|ATTRIBUTION-NONCOMMERCIAL-NO-DERIVATIVE|ATTRIBUTION-NONCOMMERCIAL-NODERIVATIVES|ATTRIBUTION-NONCOMMERCIAL-NODERIVS|NAMENSNENNUNG-NICHTKOMMERZIELL-KEINEBEARBEITUNG|NAMENSNENNUNG-KEINE-KOMMERZIELLE-NUTZUNG", the_text) ~ "CC-BY-NC-ND",
    grepl("ADS|ARCHAEOLOGYDATASERVICE", the_text) ~ "ADS",
    grepl("CC0|CC-0|ZERO", the_text) ~ "CC-0"
  )) %>% 
  select(cc, n) %>% 
  filter(!is.na(cc)) %>% 
  group_by(cc) %>% 
  tally(sort = TRUE) %>% 
  mutate(perc = round(nn / sum(nn) * 100, 0))

# what proportion is ADS or others?
rights_ads_perc <- 
rights_summary %>% 
  filter(cc == "ADS") %>% 
  pull(perc)

rights_ads_n <- 
rights_summary %>% 
  filter(cc == "ADS") %>% 
  pull(nn)

rights_eu_perc <- 
rights_summary %>% 
  filter(cc == "EU Open Access") %>% 
  pull(perc)

rights_eu_n <- 
rights_summary %>% 
  filter(cc == "EU Open Access") %>% 
  pull(nn)
  

```


Our third investigation of archaeologists' data sharing behaviours is an analysis of all records (n = `r prettyNum(nrow(dc_arch), big.mark = ",")`, as of August 2017) of archaeological data files in online repositories tracked by DataCite (an organisation that provides persistent identifiers, or DOIs for research data). The majority of records come from the Archaeology Data Service (ADS) in the UK, followed by the Dutch Archiving and Network Services (DANS-EASY). Although the earliest data dates to `r first_date`, there was a steady increase in the amount of data deposited during 1990-2008, as personal computers become more affordable and data easier to digitize. There is a substantial increase in 2009 when the DANS-EASY repository saw in influx of records from Dutch CRM firms. This relates to the introduction of Dutch archaeology legislation, the _Wet op de archeologische monumentenzorg_ (Wamz), in 2007 that formally obligated archaeologists in the Netherlands to archive their data via DANS within two years of completion of their projects (Keers et al. 2011). After 2009 the volume of deposits declined in many repositories, with the exception of generic repositories such as Figshare, Zenodo and ResearchGate, which continued to grow.

Of the `r prettyNum(sum(rights_summary$nn), big.mark = ",")` records with copyright information, the majority are released under the ADS conditions of use (n = `r prettyNum(rights_ads_n, big.mark = ",")`, `r rights_ads_perc`%), which is broadly similar to the CC-BY-NC (Moore and Richards 2015). The next most frequently used condition is the EU Open Access licence (n = `r prettyNum(rights_eu_n, big.mark = ",")`, `r rights_eu_perc`%), similar to the CC-BY licence. Creative Commons licenses such as CC-BY, CC-0 and others are used by less than 3% of records. 

Many of the records are grey literature such as unpublished reports, produced by commercial consultants or research projects. Of the `r prettyNum(number_of_records_with_file_type, big.mark = ",")` (`r perc_of_records_with_file_type`%) records that include information about the file types in the deposit, `r perc_only_pdf`% (n = `r prettyNum(n_only_pdf, big.mark = ",")`) contain only PDFs. Structured data files such as spreadsheets, databases or shapefiles are found either alone or together with other file types (typically PDFs) in `r perc_structured_data_file`% (n = `r prettyNum(n_structured_data_file, big.mark = ",")`) records. The highest proportion of records with structured data files are found in the DANS-EASY repository (n = `r prettyNum(n_struc_in_dans, big.mark = ",")`, `r perc_struc_in_dans`%), followed by the ADS with `r perc_struc_in_ads`% (n = `r n_struc_in_ads`). Note that we do not have any file format information for all the records from some smaller repositories in this sample, so it is possible that the true proportion of structured files in these records is higher. 

```{r eval = TRUE}
datacentre_plot <- 
  datacentre %>% 
  mutate(Var1 = str_wrap(Var1, 20)) %>% 
  slice(1:10) %>% 
ggplot(aes(reorder(Var1, n),
           n)) +
  geom_col() +
  coord_flip() +
  xlab("Data centre") +
  ylab("") +
  theme_new() +
  theme(axis.text.y  = element_text(size = 8)) 

allocator_plot <- 
ggplot(allocator[1:10,],
       aes(reorder(Var1, n),
           n)) +
  geom_col() +
  coord_flip() +
  theme_new()

creator_plot <- 
ggplot(creator[1:30,],
       aes(reorder(Var1, n),
           n)) +
  geom_col() +
  coord_flip() +
  theme_new()

language_plot <- 
  language %>% 
  mutate(Var2 = ifelse(grepl("en|En", Var1), "en", Var1)) %>% 
  mutate(Var2 = ifelse(grepl("de|ger", Var2), "de", Var2)) %>% 
  group_by(Var2) %>% 
  tally(sort = TRUE) %>% 
  slice(1:5) %>% 
ggplot(aes(reorder(Var2, nn),
           nn)) +
  geom_col() +
  coord_flip()  +
  theme_new() +
  xlab("Language") +
  ylab("")

contributor_plot <- 
ggplot(contributor[1:20,],
       aes(reorder(Var1, n),
           n)) +
  geom_col() +
  coord_flip()  +
  theme_new()

rights_plot <- 
ggplot(rights_summary,
       aes(reorder(cc, nn),
           nn)) +
  geom_col() +
  coord_flip()  +
  theme_new() +
  xlab("License") +
  ylab("")


publisher_plot <- 
ggplot(publisher[1:10,],
       aes(reorder(Var1, n),
           n)) +
  geom_col() +
  coord_flip() +
  theme_new()

# show plots
datacentre_plot
# allocator_plot
# creator_plot
language_plot
# contributor_plot
rights_plot
# publisher_plot
  
```


```{r dates, eval = TRUE}
# why so many in 2009? http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0078080 it's the Dutch

top_per_year <- 
dates %>% 
  group_by(yyyy_date, datacentre) %>% 
   tally(sort = TRUE)  %>% 
   top_n(n=1) 

why_2013 <- 
  xx %>% 
  filter(as.numeric(substr(date, 1,4)) == 2013)  %>% 
  group_by(datacentre) %>% 
  tally(sort = TRUE)

why_2012 <- 
  xx %>% 
  filter(as.numeric(substr(date, 1,4)) == 2012)  %>% 
  group_by(datacentre) %>% 
  tally(sort = TRUE)

why_2011 <- 
  xx %>% 
  filter(as.numeric(substr(date, 1,4)) == 2011)  %>% 
  group_by(datacentre) %>% 
  tally(sort = TRUE)

why_2010 <- 
xx %>% 
  filter(as.numeric(substr(date, 1,4)) == 2010)  %>% 
  group_by(datacentre) %>% 
  tally(sort = TRUE)

why_2009 <- 
  xx %>% 
  filter(as.numeric(substr(date, 1,4)) == 2009)  %>% 
  group_by(datacentre) %>% 
  tally(sort = TRUE)

why_2008 <- 
  xx %>% 
  filter(as.numeric(substr(date, 1,4)) == 2008)  %>% 
  group_by(datacentre) %>% 
  tally(sort = TRUE)

# in 2009, among the Dutch, what is going on?
dutch_2009 <- 
dc_arch %>% 
  filter(yyyy_date == 2009,
         datacentre == "DELFT.EASY - DANS-EASY") %>% 
  group_by(publisher) %>% 
  tally(sort=TRUE) 

# how about in ADS?
ads_2009 <- 
dc_arch %>% 
  filter(yyyy_date == 2009,
         grepl("ADS", datacentre)) %>% 
  group_by(creator) %>% 
  tally(sort=TRUE) 

# compare to 2009
dutch_2008 <- 
dc_arch %>% 
  filter(yyyy_date == 2008,
         datacentre == "DELFT.EASY - DANS-EASY") %>% 
  group_by(publisher) %>% 
  tally(sort=TRUE) 
```
  

```{r}
# panel of plots
# combine plots for lang and data center
datacentre_and_langugage_plot <- 
  datacentre_plot + annotation_custom(ggplotGrob(language_plot),
                                      xmin = 1.15, 
                                      xmax = 6.15,
                                      ymin = 10000,
                                      ymax = 40000
  )

# combine plots for lisc and year
rights_and_date_plot <- 
  rights_plot + annotation_custom(ggplotGrob(date_plot),
                                      xmin = 1, 
                                      xmax = 5,
                                      ymin = 10000,
                                      ymax = 40000
  )

library(gridExtra)
lay <- rbind(c(1,2),
             c(1,2))
grid.arrange(datacentre_and_langugage_plot,
             rights_and_date_plot,
             # datacentre_plot,
             # allocator_plot,
             # creator_plot,
             # contributor_plot,
             # publisher_plot,
             # rights_plot,
             # language_plot,
             # date_plot,
             layout_matrix = lay)
             
```


```{r eval = TRUE}

# look at some relationships
datacentre_rights_table <- 
dc_arch %>% 
  mutate(the_text =  toupper(gsub(" |_|\\(|\\)", "-", rights)),
         the_text = gsub("(\\-)\\1+", "\\1", the_text),
         the_text = gsub("-$", "", the_text)) %>% 
  mutate(cc = case_when(
    grepl("SEMANTICS/OPENACCESS", the_text) ~ "EU Open Access",
    grepl("SEMANTICS/RESTRICTEDACCESS", the_text) ~ "EU Restricted Access",
    grepl("SEMANTICS/CLOSEDACCESS", the_text) ~ "EU Closed Access",
    grepl("CC-BY-[^NC-ND]|CC-BY$|ATTRIBUTION-LICENSE|ATTRIBUTION-[^NON]", the_text) ~ "CC-BY",
    grepl("CC-BY-NC-[^ND]|BY-NC-[^ND]|CC-NC$|ATTRIBUTION-NON-COMMERCIAL-", the_text) ~ "CC-BY-NC",
    grepl("CC-BY-NC-SA-[^ND]|CC-NC-SA$|ATTRIBUTION-NON-COMMERCIAL-SHARE", the_text) ~ "CC-BY-NC-SA",
    grepl("CC-BY-NC-ND|BY-NC-ND|ATTRIBUTION-NONCOMMERCIAL-NO-DERIVATIVE|ATTRIBUTION-NONCOMMERCIAL-NODERIVATIVES|ATTRIBUTION-NONCOMMERCIAL-NODERIVS|NAMENSNENNUNG-NICHTKOMMERZIELL-KEINEBEARBEITUNG|NAMENSNENNUNG-KEINE-KOMMERZIELLE-NUTZUNG", the_text) ~ "CC-BY-NC-ND",
    grepl("ADS|ARCHAEOLOGYDATASERVICE", the_text) ~ "ADS",
    grepl("CC0|CC-0|ZERO", the_text) ~ "CC-0"
  )) %>% 
  group_by(datacentre, cc) %>% 
  filter(!is.na(cc))  %>% 
  tally(sort = TRUE) 

datacentre_rights_table[1:11,] %>% 
  ggplot(aes(cc, datacentre)) + 
  geom_point(aes(size = n),
             colour = "grey80") +
  theme_bw() + 
  xlab("") + 
  ylab("") +
  scale_size_continuous(range=c(5,25)) + 
  geom_text(aes(label = n)) +
  theme(legend.position = "none") 

# changes over time
library(ggrepel)

datacentre_by_year <- 
dc_arch %>% 
  group_by(yyyy_created, datacentre) %>% 
  tally(sort = TRUE) %>% 
  ungroup() %>% 
  filter(n > 150)


  ggplot(datacentre_by_year, 
         aes(yyyy_created,
             n,
             colour = datacentre)) +
  geom_line(size = 2) +
  theme_bw()  +
  #scale_y_log10() +
  theme(legend.position = "none")  +
  facet_wrap(~ datacentre, scales = "free_y")

```


```{r eval=FALSE}
# can we scrape google scholar for citation data?
# no, they are very sensitive to robots!

base_url <- "https://scholar.google.com/scholar?q="
doi <- dc_arch$doi
the_url <- paste0(base_url,doi)

# output storage
title_of_citing_item <-  vector("list", length(the_url))
title_of_no_items <- vector("list", length(the_url))

for(i in seq_along(the_url)){
  
library(rvest) 
the_content <- httr::GET(the_url[[i]],
                         user_agent(paste0(sample(letters[1:10]), collapse = "")))
the_content <- read_html(the_content)

title_of_citing_item[[i]] <- 
the_content %>% 
html_nodes(".gs_rt a") %>%
  html_text() 

title_of_no_items[[i]] <- 
the_content %>% 
html_nodes("p") %>%
  html_text()

print(paste0(i, " ", title_of_citing_item[[i]], " ", title_of_no_items[[i]]))
Sys.sleep(sample(10, 1) * 0.5)

}

saveRDS(title_of_citing_item, "../data/raw_data/title_of_citing_item.rds")
saveRDS(title_of_no_items, "../data/raw_data/title_of_no_items.rds")

# there are none?
idx <- unlist(map(title_of_citing_item, ~!identical(.x, character(0))))
sum(idx)
title_of_citing_item[idx]

title_of_no_items

idx <- unlist(map(title_of_no_items, ~!identical(.x, character(0))))
sum(idx)
title_of_no_items[idx]

```

```{r eval = FALSE}
# This also doesn't work
require(RSelenium)

# vignette("RSelenium-docker", package = "RSelenium")

# docker run -d -p 4445:4444 selenium/standalone-firefox:2.53.0
# docker-machine ip
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100",
                      port = 4445L)

base_url <- "https://scholar.google.com/scholar?q="
doi <- dc_arch$doi
the_url <- paste0(base_url,doi)

remDr$navigate(the_url[i])

remDr$getPageSource() %>% 
  unlist() %>% 
  read_html() %>% 
  html_nodes(".gs_rt a") %>%
  html_text() 

remDr$getPageSource() %>% 
  unlist() %>% 
  read_html() %>% 
  html_nodes(".gs_med") %>%
  html_text() 

remDr$getPageSource() %>% 
  unlist() %>% 
  read_html() %>% 
  html_nodes("p") %>%
  html_text() 
 


```



### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, cache = FALSE, eval = TRUE}
# which R packages and versions?
devtools::session_info()
```

The current Git commit details are:

```{r eval = TRUE}
# what commit is this file at? You may need to change the path value
# if your Rmd is not in analysis/paper/
git2r::repository("../..")
```

